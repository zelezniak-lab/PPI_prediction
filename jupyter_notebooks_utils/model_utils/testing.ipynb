{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c53ad7d9-7c3d-43c6-935f-62f2737961d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 1.11.0+cu113\n",
      "Uninstalling torch-1.11.0+cu113:\n",
      "  Would remove:\n",
      "    /home/danish/.local/bin/convert-caffe2-to-onnx\n",
      "    /home/danish/.local/bin/convert-onnx-to-caffe2\n",
      "    /home/danish/.local/bin/torchrun\n",
      "    /home/danish/.local/lib/python3.10/site-packages/caffe2/*\n",
      "    /home/danish/.local/lib/python3.10/site-packages/torch-1.11.0+cu113.dist-info/*\n",
      "    /home/danish/.local/lib/python3.10/site-packages/torch/*\n",
      "Proceed (Y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(1000000)\n",
    "!pip3 uninstall torch torchvision torchaudio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc49a3f5-c1a4-4d6c-921d-4dba686ff824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19566"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id =[]\n",
    "sequence = []\n",
    "name_to_seq_dict = {}\n",
    "from Bio import SeqIO\n",
    "for seq_record in SeqIO.parse(\"9606.protein.sequences.v11.5.fa\", \"fasta\"):\n",
    "    id.append(seq_record.id)\n",
    "    x = repr(seq_record.seq)[5:-2]\n",
    "    l = \"\"\n",
    "    for i in x:\n",
    "        l = l + i + \" \"\n",
    "    l = l.rstrip()\n",
    "    \n",
    "    sequence.append(l)\n",
    "name_to_seq_dict = dict(zip(id, sequence))\n",
    "name_to_seq_dict['9606.ENSP00000002829']\n",
    "len(name_to_seq_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "956ae79c-65fe-4de2-b2a0-f71edda7b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ppi_dataset_from_positive_pair_string.csv\")\n",
    "data.head()\n",
    "data['protein1_sequence'] = data.apply (lambda row: name_to_seq_dict[row['protein1']], axis=1)\n",
    "data['protein2_sequence'] = data.apply (lambda row: name_to_seq_dict[row['protein2']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bd07533-8cc5-471b-90c8-36c534f7099d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# print('Device name:', torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ae8c5c5-d7cd-4868-873d-61c1bd91888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = pd.DataFrame()\n",
    "data = data[['protein1_sequence','protein2_sequence','label']]\n",
    "data['concat_protein'] = data['protein1_sequence'] +\" \" + data['protein2_sequence']\n",
    "data_2['concat_protein'] = + data['protein2_sequence']+\" \" + data['protein1_sequence'] \n",
    "data_2['label'] = data['label']\n",
    "data = data[['concat_protein','label']]\n",
    "data_2 = data_2[['concat_protein','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fe8660d-626a-4cf0-876a-d729c2edc274",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.concat([data,data_2]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "153b7b36-78fa-4968-bfe9-c283dfe2cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X  = final_data.drop(['label'],axis=1).values\n",
    "y = final_data['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "372a6bc2-9719-4344-b728-28582d870bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert_bfd', do_lower_case=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f554149a-1623-419d-8cb9-3f63efa11e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(protein_sequences,MAX_LEN = 122):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for protein_sequence in tqdm(protein_sequences):\n",
    "        id = tokenizer.batch_encode_plus(protein_sequence ,max_length = MAX_LEN, add_special_tokens=True, pad_to_max_length=True)\n",
    "        input_id = id.get('input_ids')\n",
    "        attention_mask = id.get('attention_mask')\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "    \n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c963eb9-0249-406f-a08b-528804e5fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "y_train = torch.tensor(y_train)\n",
    "y_test = torch.tensor(y_test)\n",
    "y_test = y_test[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eab536e0-fa72-41d7-a231-76c80f81a390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/danish/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1705.01it/s]\n"
     ]
    }
   ],
   "source": [
    "test_inputs,test_masks = preprocessing_for_bert(X_test[0:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a015581-2546-497b-9813-ed797cbbc57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "# train_data = TensorDataset(train_inputs, train_masks,y_train)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "# train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size,drop_last=True)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks,y_test)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "532d290a-e875-4099-969c-de64f146187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a51b711-10a3-465a-b53a-7bce52266057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb4740af-f17f-4795-bd14-481109ec69d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim = 1024):\n",
    "        super(BertClassifier,self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"Rostlab/prot_bert_bfd\")\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(embed_dim,512)\n",
    "        self.fc2 = nn.Linear(512,256)\n",
    "        \n",
    "        self.fc3 = nn.Linear(512,64)\n",
    "        self.fc4 = nn.Linear(64,32)\n",
    "        \n",
    "        self.fc5 = nn.Linear(32,16)\n",
    "        self.fc6 = nn.Linear(16,8)\n",
    "        self.fc7 = nn.Linear(8,1)\n",
    "\n",
    "    \n",
    "    def forward(self, input_ids,attention_mask):\n",
    "        \n",
    "        embedding = self.bert(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        output = torch.tensor(embedding[0][:, 0, :])\n",
    "        # print(output)\n",
    "        output_1 = self.relu(self.fc1(output))\n",
    "        output_2 = self.relu(self.fc3(output_1))\n",
    "        output_3 = self.relu(self.fc4(output_2))\n",
    "        output_4 = self.relu(self.fc5(output_3))\n",
    "        output_5 = self.relu(self.fc6(output_4))\n",
    "        output = self.fc7(output_5)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57e11cd6-63a5-49f8-a48d-25e65f8f8e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def initilize_model(learning_rate=0.001):\n",
    "    \"\"\"Instantiate a CNN model and an optimizer.\"\"\"\n",
    "\n",
    "    model = BertClassifier()\n",
    "    \n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    # Send model to `device` (GPU/CPU)\n",
    "    model.to(device)\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92927d-7d6a-48b2-bfa2-150f54874be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 15.90 GiB total capacity; 845.42 MiB already allocated; 18.56 MiB free; 860.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m initilize_model(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_mode_trained_after_37_epochs.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:1046\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1044\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1045\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1046\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:1016\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1015\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1016\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:1001\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    997\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39m_UntypedStorage)\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;66;03m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m loaded_storages[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39m_TypedStorage(\n\u001b[0;32m-> 1001\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1002\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:176\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 176\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:158\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m storage_type(obj\u001b[38;5;241m.\u001b[39mnbytes())\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_utils.py:79\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     new_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:661\u001b[0m, in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m _lazy_init()\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m# We may need to call lazy init again if we are a forked child\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# del _CudaBase.__new__\u001b[39;00m\n\u001b[0;32m--> 661\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_CudaBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 15.90 GiB total capacity; 845.42 MiB already allocated; 18.56 MiB free; 860.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = initilize_model(learning_rate=0.01)\n",
    "checkpoint = torch.load('best_mode_trained_after_37_epochs.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e337614c-22df-40e1-9783-f3b21c7c5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "    \n",
    "val_accuracy = []\n",
    "val_loss = []\n",
    "for batch in tqdm(test_dataloader):\n",
    "        # Load batch to GPU\n",
    "    b_input_ids, b_attn_masks, b_labels = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids = b_input_ids.reshape((1,48,122)).squeeze(0)\n",
    "    b_attn_masks = b_attn_masks.reshape((1,48,122)).squeeze(0)\n",
    "    b_labels = b_labels.reshape((1,48,1)).squeeze(0)\n",
    "    with torch.no_grad():\n",
    "        logits = model( b_input_ids, b_attn_masks)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    loss = loss_fn(logits, b_labels.float())\n",
    "\n",
    "    val_loss.append(loss.item())\n",
    "# # model = BertModel.from_pretrained(\"Rostlab/prot_bert_bfd\")\n",
    "# for i in tqdm(train_dataloader):\n",
    "#     print(i)\n",
    "    preds = torch.round(torch.sigmoid(logits))\n",
    "        # print(preds,b_labels)\n",
    "    accuracy = (preds.float() == b_labels.float()).cpu().numpy().mean() * 100\n",
    "\n",
    "    val_accuracy.append(accuracy)\n",
    "    \n",
    "val_loss = np.mean(val_loss)\n",
    "val_accuracy = np.mean(val_accuracy)\n",
    "print(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a1f8d0-7826-469f-8dd7-c866bc9fbe3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d650f7-9d52-43fc-96bf-00b1b9ed0ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
