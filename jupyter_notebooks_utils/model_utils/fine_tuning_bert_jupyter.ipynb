{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "594c8ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "sys.setrecursionlimit(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deb98b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug  5 12:15:49 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.73.08    Driver Version: 510.73.08    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          On   | 00000000:17:00.0 Off |                    0 |\n",
      "|  0%   34C    P8    30W / 300W |      0MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A40          On   | 00000000:31:00.0 Off |                    0 |\n",
      "|  0%   35C    P8    33W / 300W |      0MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A40          On   | 00000000:B1:00.0 Off |                    0 |\n",
      "|  0%   33C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A40          On   | 00000000:CA:00.0 Off |                    0 |\n",
      "|  0%   34C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96068dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1b5061e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>seq_1</th>\n",
       "      <th>seq_2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>M A A V R A L V A S R L A A A S A F T S L S P ...</td>\n",
       "      <td>M A A A L A R L G L R P V K Q V R V Q F C P F ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>M A A V R A L V A S R L A A A S A F T S L S P ...</td>\n",
       "      <td>M A A A V L G Q L G A L W I H N L R S R G K L ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>M A A V R A L V A S R L A A A S A F T S L S P ...</td>\n",
       "      <td>M A A G I V A S R R L R D L L T R R L T G S N ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>M A A V R A L V A S R L A A A S A F T S L S P ...</td>\n",
       "      <td>M A G Y L R V V R S L C R A S G S R P A W A P ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>M A A V R A L V A S R L A A A S A F T S L S P ...</td>\n",
       "      <td>M A A A V V L A A G L R A A R R A V A A T G V ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              seq_1  \\\n",
       "0           0  M A A V R A L V A S R L A A A S A F T S L S P ...   \n",
       "1           1  M A A V R A L V A S R L A A A S A F T S L S P ...   \n",
       "2           2  M A A V R A L V A S R L A A A S A F T S L S P ...   \n",
       "3           3  M A A V R A L V A S R L A A A S A F T S L S P ...   \n",
       "4           4  M A A V R A L V A S R L A A A S A F T S L S P ...   \n",
       "\n",
       "                                               seq_2  label  \n",
       "0  M A A A L A R L G L R P V K Q V R V Q F C P F ...      1  \n",
       "1  M A A A V L G Q L G A L W I H N L R S R G K L ...      1  \n",
       "2  M A A G I V A S R R L R D L L T R R L T G S N ...      1  \n",
       "3  M A G Y L R V V R S L C R A S G S R P A W A P ...      1  \n",
       "4  M A A A V V L A A G L R A A R R A V A A T G V ...      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"final_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e3679f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id =[]\n",
    "# sequence = []\n",
    "# name_to_seq_dict = {}\n",
    "# from Bio import SeqIO\n",
    "# for seq_record in SeqIO.parse(\"9606.protein.sequences.v11.5.fa\", \"fasta\"):\n",
    "#     id.append(seq_record.id)\n",
    "#     x = repr(seq_record.seq)[5:-2]\n",
    "#     l = \"\"\n",
    "#     for i in x:\n",
    "#         l = l + i + \" \"\n",
    "#     l = l.rstrip()\n",
    "    \n",
    "#     sequence.append(l)\n",
    "# name_to_seq_dict = dict(zip(id, sequence))\n",
    "# name_to_seq_dict['9606.ENSP00000002829']\n",
    "# len(name_to_seq_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c7d74ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id =[]\n",
    "# sequence = []\n",
    "# name_to_seq_dict = {}\n",
    "# from Bio import SeqIO\n",
    "# for seq_record in SeqIO.parse(\"9606.protein.sequences.v11.5.fa\", \"fasta\"):\n",
    "#     id.append(seq_record.id)\n",
    "#     x = repr(seq_record.seq)[5:-2]\n",
    "#     l = \"\"\n",
    "#     for i in x:\n",
    "#         l = l + i + \" \"\n",
    "#     l = l.rstrip()\n",
    "    \n",
    "#     sequence.append(l)\n",
    "# name_to_seq_dict = dict(zip(id, sequence))\n",
    "# name_to_seq_dict['9606.ENSP00000002829']\n",
    "# len(name_to_seq_dict) data['protein1_sequence'] = data.apply (lambda row: name_to_seq_dict[row['protein1']], axis=1)\n",
    "# data['protein2_sequence'] = data.apply (lambda row: name_to_seq_dict[row['protein2']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1b59379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len  = max([len(i) for i in data['protein1_sequence'].tolist()])\n",
    "# max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1c18052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device name: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "\n",
    "print('Device name:', torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51445296",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = pd.DataFrame()\n",
    "data = data[['seq_1','seq_2','label']]\n",
    "data['concat_protein'] = data['seq_1'] +\" \" + data['seq_2']\n",
    "\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00b4e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finadata = pd.concat([data,data_2]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c7556ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    138817\n",
       "1    111183\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts()\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fb256d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M A A V R A L V A S R L A A A S A F T S L S P G G R T P S Q R A A L H L S V P R P A A R V A L V L S G C G V Y D G T E I H E A S A I L V H L S R G G A E V Q I F A P D V P Q M H V I D H T K G Q P S E G E S R N V L T E S A R I A R G K I T D L A N L S A A N H D A A I F P G G F G A A K N L S T F A V D G K D C K V N K E V E R V L K E F H Q A G K P I G L C C I A P V L A A K V L R G V E V T V G H E Q E E G G K W P Y A G T A E A I K A L G A K H C V K E V V E A H V D Q K N K V V T T P A F M C E T A L H Y I H D G I G A M V R K V L E L T G K M A A A L A R L G L R P V K Q V R V Q F C P F E K N V E S T R T F L Q T V S S E K V R S T N L N C S V I A D V R H D G S E P C V D V L F G D G H R L I M R G A H L T A L E M L T A F A S H I R A R D A A G S G D K P G A D T G R'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['concat_protein'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7e02c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len  = max([len(i) for i in data['concat_protein'].tolist()])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11fdb7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_1</th>\n",
       "      <th>seq_2</th>\n",
       "      <th>label</th>\n",
       "      <th>concat_protein</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119604</th>\n",
       "      <td>M S T T V N V D S L A E Y E K S Q I K R A L E ...</td>\n",
       "      <td>M R P S G T A G A A L L A L L A A L C P A S R ...</td>\n",
       "      <td>1</td>\n",
       "      <td>M S T T V N V D S L A E Y E K S Q I K R A L E ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142298</th>\n",
       "      <td>M E R K V L A L Q A R K K R T K A K K D K A Q ...</td>\n",
       "      <td>M A T T A T M A T S G S A R K R L L K E E D M ...</td>\n",
       "      <td>1</td>\n",
       "      <td>M E R K V L A L Q A R K K R T K A K K D K A Q ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476983</th>\n",
       "      <td>M A A S R L E L N L V R L L S R C E A M A A E ...</td>\n",
       "      <td>M V L L T M I A R V A D G L P L A A S M Q E D ...</td>\n",
       "      <td>1</td>\n",
       "      <td>M A A S R L E L N L V R L L S R C E A M A A E ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7193</th>\n",
       "      <td>M G K G D P K K P R G K M S S Y A F F V Q T C ...</td>\n",
       "      <td>M P S R L R K T R K L R G H V S H G H G R I G ...</td>\n",
       "      <td>1</td>\n",
       "      <td>M G K G D P K K P R G K M S S Y A F F V Q T C ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287842</th>\n",
       "      <td>M G K S L S H L P L H S S K E D A Y D G V T S ...</td>\n",
       "      <td>M D D L D A L L A D L E S T T S H I S K R P V ...</td>\n",
       "      <td>0</td>\n",
       "      <td>M G K S L S H L P L H S S K E D A Y D G V T S ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515532</th>\n",
       "      <td>M L L L G L L L L L P L L A G A R L L W N W W ...</td>\n",
       "      <td>M V N S C C G S V C S D Q G C G L E N C C R P ...</td>\n",
       "      <td>1</td>\n",
       "      <td>M L L L G L L L L L P L L A G A R L L W N W W ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454235</th>\n",
       "      <td>M P G I V E L P T L E E L K V D E V K I S S A ...</td>\n",
       "      <td>M W V T K L L P A L L L Q H V L L H L L L L P ...</td>\n",
       "      <td>0</td>\n",
       "      <td>M P G I V E L P T L E E L K V D E V K I S S A ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33185</th>\n",
       "      <td>M S G F E N L N T D F Y Q T S Y S I D D Q S Q ...</td>\n",
       "      <td>M E L Q D P K M N G A L P S D A V G Y R Q E R ...</td>\n",
       "      <td>1</td>\n",
       "      <td>M S G F E N L N T D F Y Q T S Y S I D D Q S Q ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497038</th>\n",
       "      <td>M T K K R R N N G R A K K G R G H V Q P I R C ...</td>\n",
       "      <td>M S V P A F I D I S E E D Q A A E L R A Y L K ...</td>\n",
       "      <td>1</td>\n",
       "      <td>M T K K R R N N G R A K K G R G H V Q P I R C ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455177</th>\n",
       "      <td>M A A P Q D L D I A V W L A T V H L E Q Y A D ...</td>\n",
       "      <td>M G S N S S R I G D L P K N E Y L K K L S G T ...</td>\n",
       "      <td>0</td>\n",
       "      <td>M A A P Q D L D I A V W L A T V H L E Q Y A D ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    seq_1  \\\n",
       "119604  M S T T V N V D S L A E Y E K S Q I K R A L E ...   \n",
       "142298  M E R K V L A L Q A R K K R T K A K K D K A Q ...   \n",
       "476983  M A A S R L E L N L V R L L S R C E A M A A E ...   \n",
       "7193    M G K G D P K K P R G K M S S Y A F F V Q T C ...   \n",
       "287842  M G K S L S H L P L H S S K E D A Y D G V T S ...   \n",
       "...                                                   ...   \n",
       "515532  M L L L G L L L L L P L L A G A R L L W N W W ...   \n",
       "454235  M P G I V E L P T L E E L K V D E V K I S S A ...   \n",
       "33185   M S G F E N L N T D F Y Q T S Y S I D D Q S Q ...   \n",
       "497038  M T K K R R N N G R A K K G R G H V Q P I R C ...   \n",
       "455177  M A A P Q D L D I A V W L A T V H L E Q Y A D ...   \n",
       "\n",
       "                                                    seq_2  label  \\\n",
       "119604  M R P S G T A G A A L L A L L A A L C P A S R ...      1   \n",
       "142298  M A T T A T M A T S G S A R K R L L K E E D M ...      1   \n",
       "476983  M V L L T M I A R V A D G L P L A A S M Q E D ...      1   \n",
       "7193    M P S R L R K T R K L R G H V S H G H G R I G ...      1   \n",
       "287842  M D D L D A L L A D L E S T T S H I S K R P V ...      0   \n",
       "...                                                   ...    ...   \n",
       "515532  M V N S C C G S V C S D Q G C G L E N C C R P ...      1   \n",
       "454235  M W V T K L L P A L L L Q H V L L H L L L L P ...      0   \n",
       "33185   M E L Q D P K M N G A L P S D A V G Y R Q E R ...      1   \n",
       "497038  M S V P A F I D I S E E D Q A A E L R A Y L K ...      1   \n",
       "455177  M G S N S S R I G D L P K N E Y L K K L S G T ...      0   \n",
       "\n",
       "                                           concat_protein  \n",
       "119604  M S T T V N V D S L A E Y E K S Q I K R A L E ...  \n",
       "142298  M E R K V L A L Q A R K K R T K A K K D K A Q ...  \n",
       "476983  M A A S R L E L N L V R L L S R C E A M A A E ...  \n",
       "7193    M G K G D P K K P R G K M S S Y A F F V Q T C ...  \n",
       "287842  M G K S L S H L P L H S S K E D A Y D G V T S ...  \n",
       "...                                                   ...  \n",
       "515532  M L L L G L L L L L P L L A G A R L L W N W W ...  \n",
       "454235  M P G I V E L P T L E E L K V D E V K I S S A ...  \n",
       "33185   M S G F E N L N T D F Y Q T S Y S I D D Q S Q ...  \n",
       "497038  M T K K R R N N G R A K K G R G H V Q P I R C ...  \n",
       "455177  M A A P Q D L D I A V W L A T V H L E Q Y A D ...  \n",
       "\n",
       "[250000 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data =data.sample(n=250000)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e322f886",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "463c5b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X  = data['concat_protein']\n",
    "y = data['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "716de181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187500"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b664d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "451bbc8c",
   "metadata": {},
   "source": [
    "# Preprocessing for Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e167271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert_bfd', do_lower_case=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04997451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x= [tokenizer.encode(i,add_special_tokens = True) for i in data['concat_protein'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad1ebbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len = max([len(i) for i in x])\n",
    "# max_len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "409113b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(protein_sequences):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for protein_sequence in tqdm(protein_sequences):\n",
    "        id = tokenizer.encode_plus(protein_sequence,max_length = 1000,pad_to_max_length = True, truncation=False)\n",
    "        input_id = id.get('input_ids')\n",
    "        attention_mask = id.get('attention_mask')\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)                         \n",
    "    \n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0db82563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "y_train = torch.tensor(y_train)\n",
    "y_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "615dba53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/187500 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/cephyr/users/anwer/Alvis/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2323: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 187500/187500 [04:04<00:00, 767.11it/s]\n",
      "100%|██████████| 62500/62500 [01:20<00:00, 776.46it/s]\n"
     ]
    }
   ],
   "source": [
    "train_inputs,train_masks = preprocessing_for_bert(X_train)\n",
    "test_inputs,test_masks = preprocessing_for_bert(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "907be1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "train_data = TensorDataset(train_inputs, train_masks,y_train)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size,drop_last=True)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks,y_test)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88ea9957",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import BertModel, BertTokenizer\n",
    "# # model = BertModel.from_pretrained(\"Rostlab/prot_bert_bfd\")\n",
    "# for i in tqdm(train_dataloader):\n",
    "#     print(i)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efff5f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# for step,batch in enumerate(train_dataloader):\n",
    "            \n",
    "#     b_input_ids, b_attn_masks, b_labels = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    \n",
    "#     b_input_ids = b_input_ids.reshape((1,2,122)).squeeze(0).to(device)\n",
    "#     b_attn_masks = b_attn_masks.reshape((1,2,122)).squeeze(0).to(device)\n",
    "#     b_labels = b_labels.reshape((1,2,1)).squeeze(0).to(device)\n",
    "#     embedding = model(input_ids=b_input_ids,attention_mask=b_attn_masks).to(device)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba8834f",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a885206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e099089",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim = 1024):\n",
    "        super(BertClassifier,self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"Rostlab/prot_bert_bfd\")\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(embed_dim,512)\n",
    "        self.fc2 = nn.Linear(512,256)\n",
    "        \n",
    "        self.fc3 = nn.Linear(512,64)\n",
    "        self.fc4 = nn.Linear(64,32)\n",
    "        \n",
    "        self.fc5 = nn.Linear(32,16)\n",
    "        self.fc6 = nn.Linear(16,8)\n",
    "        self.fc7 = nn.Linear(8,1)\n",
    "\n",
    "    \n",
    "    def forward(self, input_ids,attention_mask):\n",
    "        \n",
    "        embedding = self.bert(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        output = torch.tensor(embedding[0][:, 0, :])\n",
    "        # print(output)\n",
    "        output_1 = self.relu(self.fc1(output))\n",
    "        output_2 = self.relu(self.fc3(output_1))\n",
    "        output_3 = self.relu(self.fc4(output_2))\n",
    "        output_4 = self.relu(self.fc5(output_3))\n",
    "        output_5 = self.relu(self.fc6(output_4))\n",
    "        output = self.fc7(output_5)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6eb5516",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = BertClassifier()\n",
    "# model.to(device)\n",
    "# for step,batch in enumerate(train_dataloader):\n",
    "#     b_input_ids, b_attn_masks, b_labels = tuple(t.to(device) for t in batch)\n",
    "#     b_input_ids = b_input_ids.reshape((1,16,122)).squeeze(0)\n",
    "#     b_attn_masks = b_attn_masks.reshape((1,16,122)).squeeze(0)\n",
    "#     print( b_input_ids)\n",
    "#     b_labels = b_labels.reshape((1,16,1)).squeeze(0)\n",
    "#             # print(b_input_1)\n",
    "#     print(b_input_ids.shape)\n",
    "#     logits = model(b_input_ids, b_attn_masks)        \n",
    "            \n",
    "#     print(logits)\n",
    "#     break\n",
    "\n",
    "    \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0c18ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.bert.embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4095d61",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b1f6889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def initilize_model(learning_rate=0.001):\n",
    "    \"\"\"Instantiate a CNN model and an optimizer.\"\"\"\n",
    "\n",
    "    model = BertClassifier()\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Send model to `device` (GPU/CPU)\n",
    "    model.to(device)\n",
    "    model= nn.DataParallel(model,device_ids = [0,1,2,3])\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f2c8de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "def train(model, optimizer, train_dataloader, val_dataloader = None, epochs =10):\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Start training...\\n\")\n",
    "    \n",
    "    # for param in model.bert.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    for epoch_i in range(1,epochs):\n",
    "        \n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        for step,batch in tqdm(enumerate(train_dataloader)):\n",
    "            \n",
    "            b_input_ids, b_attn_masks, b_labels = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids = b_input_ids.reshape((1,24,1000)).squeeze(0)\n",
    "            b_attn_masks = b_attn_masks.reshape((1,24,1000)).squeeze(0)\n",
    "            b_labels = b_labels.reshape((1,24,1)).squeeze(0)\n",
    "            # print(b_input_1)\n",
    "            model.zero_grad()\n",
    "            \n",
    "            logits = model( b_input_ids, b_attn_masks)\n",
    "            \n",
    "            \n",
    "            \n",
    "#             print(b_labels)\n",
    "#             print(logits)\n",
    "            loss = loss_fn(logits,b_labels.float()) \n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.mean().backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "               \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        \n",
    "    \n",
    "        if val_dataloader is not None:\n",
    "                \n",
    "                val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "                   \n",
    "                if val_accuracy > best_accuracy:\n",
    "                    best_accuracy = val_accuracy\n",
    "                    torch.save({\n",
    "                        'epoch': epoch_i + 1,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': loss_fn,\n",
    "                        }, 'best_mode_trained_after_37_epochs.pth')\n",
    "\n",
    "                # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max',patience = 3)\n",
    "                # scheduler.step(val_accuracy)\n",
    "        print(f\"Epoch: {epoch_i} | Training Loss: {avg_train_loss}  | Validation Loss: {val_loss}  | Accuracy: {val_accuracy:.2f}\")\n",
    "        with open('result.txt', 'a') as f:\n",
    "            print(f\"Epoch: {epoch_i} | Training Loss: {avg_train_loss}  | Validation Loss: {val_loss}  | Accuracy: {val_accuracy:.2f}\", file=f) \n",
    "    print(\"\\n\")\n",
    "    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n",
    "\n",
    "            \n",
    "   \n",
    "\n",
    "\n",
    "def evaluate(model,val_dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_masks, b_labels = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids = b_input_ids.reshape((1,24,1000)).squeeze(0)\n",
    "        b_attn_masks = b_attn_masks.reshape((1,24,1000)).squeeze(0)\n",
    "        b_labels = b_labels.reshape((1,24,1)).squeeze(0)\n",
    "        with torch.no_grad():\n",
    "                logits = model( b_input_ids, b_attn_masks)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        loss = loss_fn(logits, b_labels.float())\n",
    "\n",
    "        val_loss.append(loss.item())\n",
    "# # model = BertModel.from_pretrained(\"Rostlab/prot_bert_bfd\")\n",
    "# for i in tqdm(train_dataloader):\n",
    "#     print(i)\n",
    "        preds = torch.round(torch.sigmoid(logits))\n",
    "        # print(preds,b_labels)\n",
    "        accuracy = (preds.float() == b_labels.float()).cpu().numpy().mean() * 100\n",
    "\n",
    "        val_accuracy.append(accuracy)\n",
    "    \n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00cb5d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = initilize_model(learning_rate=0.01)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), nesterov = True, lr=0.001, momentum=0.9)\n",
    "# checkpoint = torch.load('best_model.pth')\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# # model = nn.DataParallel(model)\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdf3cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint['model_state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c675744d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IncompatibleKeys(missing_keys, unexpected_keys)# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b29f6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-26-923b37ccdfc9>:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output = torch.tensor(embedding[0][:, 0, :])\n",
      "2639it [27:50,  1.53it/s]"
     ]
    }
   ],
   "source": [
    "train(model, optimizer,train_dataloader,test_dataloader,epochs=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c753f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bb9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "    \n",
    "val_accuracy = []\n",
    "val_loss = []\n",
    "for batch in tqdm(test_dataloader):\n",
    "        # Load batch to GPU\n",
    "    b_input_ids, b_attn_masks, b_labels = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids = b_input_ids.reshape((1,48,122)).squeeze(0)\n",
    "    b_attn_masks = b_attn_masks.reshape((1,48,122)).squeeze(0)\n",
    "    b_labels = b_labels.reshape((1,48,1)).squeeze(0)\n",
    "    with torch.no_grad():\n",
    "        logits = model( b_input_ids, b_attn_masks)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    loss = loss_fn(logits, b_labels.float())\n",
    "\n",
    "    val_loss.append(loss.item())\n",
    "# # model = BertModel.from_pretrained(\"Rostlab/prot_bert_bfd\")\n",
    "# for i in tqdm(train_dataloader):\n",
    "#     print(i)\n",
    "    preds = torch.round(torch.sigmoid(logits))\n",
    "        # print(preds,b_labels)\n",
    "    accuracy = (preds.float() == b_labels.float()).cpu().numpy().mean() * 100\n",
    "\n",
    "    val_accuracy.append(accuracy)\n",
    "    \n",
    "val_loss = np.mean(val_loss)\n",
    "val_accuracy = np.mean(val_accuracy)\n",
    "print(val_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d83c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9fa53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc54e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc3d493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9e572d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
