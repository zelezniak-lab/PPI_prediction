{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee9e6e12",
   "metadata": {},
   "source": [
    "### 1. Importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f87afc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anwer/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanish2562022\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import timeit\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import  confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from matplotlib.pyplot import figure\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13201b4",
   "metadata": {},
   "source": [
    "### 2. Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b86c4140",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../../csv_files_new_ppi/training_and_test_set/train_set_without_embedding.csv\")\n",
    "test =  pd.read_csv(\"../../csv_files_new_ppi/training_and_test_set/test_set_without_embedding.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "100b971e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    21347\n",
       "0    21347\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos = train[train.label == 1][0:100000]\n",
    "train_neg = train[train.label == 0][0:100000]\n",
    "train = pd.concat([train_pos,train_neg])\n",
    "\n",
    "test_pos = test[test.label == 1]\n",
    "test_neg = test[test.label ==0][0:len(test_pos)]\n",
    "test = pd.concat([test_pos,test_neg])\n",
    "test.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a875593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pos =  test[test.label == 1][0:10000]\n",
    "val_neg =  test[test.label == 0][0:10000]\n",
    "val = pd.concat([val_pos,val_neg])\n",
    "\n",
    "test_pos =  test[test.label == 1][10000:]\n",
    "test_neg =  test[test.label == 0][10000:]\n",
    "test = pd.concat([test_pos,test_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54417de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Train dataset:  200000\n",
      "Size of Test dataset:  22694\n",
      "Size of val dataset:  20000\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of Train dataset: \", len(train))\n",
    "print(\"Size of Test dataset: \", len(test))\n",
    "print(\"Size of val dataset: \", len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76efd1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative points in training set: 100000\n",
      "Number of positive points in training set: 100000\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Number of negative points in test set: 11347\n",
      "Number of positive points in test set: 11347\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Number of negative points in test set: 10000\n",
      "Number of positive points in test set: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of negative points in training set: {train.label.value_counts()[0]}\")\n",
    "print(f\"Number of positive points in training set: {train.label.value_counts()[1]}\")\n",
    "print(\"----\"*57)\n",
    "print(f\"Number of negative points in test set: {test.label.value_counts()[0]}\")\n",
    "print(f\"Number of positive points in test set: {test.label.value_counts()[1]}\")\n",
    "print(\"----\"*57)\n",
    "print(f\"Number of negative points in test set: {val.label.value_counts()[0]}\")\n",
    "print(f\"Number of positive points in test set: {val.label.value_counts()[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbb3682",
   "metadata": {},
   "source": [
    "### 3. Importing embedding vectors from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0de3e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../pickle/embedding_vectors_pickle/sum_of_amino_acid_vector.pickle\",'rb') as handle:\n",
    "    dc = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de026ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_embed(prot_name):\n",
    "    try:\n",
    "        return dc[prot_name]\n",
    "    except:\n",
    "        return np.nan\n",
    "train['embed_vec_protein_A'] = train[\"Protein_A\"].apply(return_embed)\n",
    "train['embed_vec_protein_B'] = train[\"Protein_B\"].apply(return_embed)\n",
    "\n",
    "test['embed_vec_protein_A'] = test[\"Protein_A\"].apply(return_embed)\n",
    "test['embed_vec_protein_B'] = test[\"Protein_B\"].apply(return_embed)\n",
    "\n",
    "val['embed_vec_protein_A'] = val[\"Protein_A\"].apply(return_embed)\n",
    "val['embed_vec_protein_B'] = val[\"Protein_B\"].apply(return_embed)\n",
    "\n",
    "\n",
    "train = train.dropna()\n",
    "test = test.dropna()\n",
    "val = val.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b64e9a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 200000/200000 [00:42<00:00, 4702.37it/s]\n",
      "100%|██████████████████████████████████████████████████| 22694/22694 [00:05<00:00, 4497.50it/s]\n",
      "100%|██████████████████████████████████████████████████| 20000/20000 [00:04<00:00, 4699.98it/s]\n"
     ]
    }
   ],
   "source": [
    "train_features_Protein_A = []\n",
    "train_features_Protein_B = []\n",
    "train_label = []\n",
    "test_features_Protein_A = []\n",
    "test_features_Protein_B = []\n",
    "test_label =[]\n",
    "val_features_Protein_A = []\n",
    "val_features_Protein_B = []\n",
    "val_label = []\n",
    "for i in tqdm(range(len(train))):\n",
    "    train_features_Protein_A.append(np.array(train.iloc[i].embed_vec_protein_A))\n",
    "    train_features_Protein_B.append(np.array(train.iloc[i].embed_vec_protein_B))\n",
    "    train_label.append(np.array(train.iloc[i].label))\n",
    "    \n",
    "for i in tqdm(range(len(test))):\n",
    "    test_features_Protein_A.append(np.array(test.iloc[i].embed_vec_protein_A))\n",
    "    test_features_Protein_B.append(np.array(test.iloc[i].embed_vec_protein_B))\n",
    "    test_label.append(np.array(test.iloc[i].label))  \n",
    "for i in tqdm(range(len(val))):\n",
    "    \n",
    "    \n",
    "    val_features_Protein_A.append(np.array(val.iloc[i].embed_vec_protein_A))\n",
    "    val_features_Protein_B.append(np.array(val.iloc[i].embed_vec_protein_B))\n",
    "    val_label.append(np.array(val.iloc[i].label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b1f23cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_Protein_A = np.array(train_features_Protein_A)\n",
    "train_features_Protein_B = np.array(train_features_Protein_B)\n",
    "train_label = np.array(train_label)\n",
    "\n",
    "test_features_Protein_A = np.array(test_features_Protein_A)\n",
    "test_features_Protein_B = np.array(test_features_Protein_B)\n",
    "test_label = np.array(test_label)\n",
    "\n",
    "val_features_Protein_A = np.array(val_features_Protein_A)\n",
    "val_features_Protein_B = np.array(val_features_Protein_B)\n",
    "val_label = np.array(val_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6833262",
   "metadata": {},
   "source": [
    "### 4. Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fcd522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data_A,X_data_B, y_data):\n",
    "        self.X_data_A = X_data_A\n",
    "        self.X_data_B = X_data_B\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data_A[index],self.X_data_B[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbf7455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Data(torch.FloatTensor(train_features_Protein_A), torch.FloatTensor(train_features_Protein_B),\n",
    "                       torch.FloatTensor(train_label))\n",
    "\n",
    "test_data = Data(torch.FloatTensor(test_features_Protein_A), torch.FloatTensor(test_features_Protein_B),\n",
    "                       torch.FloatTensor(test_label))\n",
    "\n",
    "val_data = Data(torch.FloatTensor(val_features_Protein_A), torch.FloatTensor(val_features_Protein_B),\n",
    "                       torch.FloatTensor(val_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0f2589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=512, shuffle=True,drop_last=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=512,shuffle=True,drop_last=True )\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=512,shuffle=True,drop_last=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ced5e5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1024])\n",
      "torch.Size([512, 1024])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "for i,j,k in train_loader:\n",
    "    print(i.shape)\n",
    "    print(j.shape)\n",
    "    print(k.shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070aa937",
   "metadata": {},
   "source": [
    "### 5. Building Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "317ccc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self,config, embed_dim =1024):\n",
    "        super(BertClassifier,self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.config = config\n",
    "        self.conv1  = nn.Conv1d(in_channels = 1,out_channels = 33, kernel_size = 3, stride=1)\n",
    "        self.fc1 = nn.Linear(33726,config.dim_1)\n",
    "        self.fully_connected_layers_1 = nn.ModuleList([nn.Linear(config.dim_1,config.dim_1)\n",
    "                                                    for _ in range(config.layer_fc_1)])\n",
    "        \n",
    "        \n",
    "    \n",
    "        self.fc_2 = nn.Linear(config.dim_1,config.dim_2)\n",
    "        self.fully_connected_layers_2 = nn.ModuleList([nn.Linear(config.dim_2,config.dim_2)\n",
    "                                                    for _ in range(config.layer_fc_2)]) \n",
    "        self.bn2 = nn.BatchNorm1d(num_features=config.dim_2)\n",
    "        self.fc3 = nn.Linear(config.dim_2,config.dim_1)\n",
    "        \n",
    "        self.fc4 = nn.Linear(config.dim_1,256)\n",
    "        self.drop = nn.Dropout(p = 0.2)\n",
    "        self.fc5 = nn.Linear(256,128)\n",
    "        self.fc6 = nn.Linear(128,64)\n",
    "        self.fc7 = nn.Linear(64,32)\n",
    "        self.fc8 = nn.Linear(32,16)\n",
    "        self.fc9 = nn.Linear(16,8)\n",
    "        self.fc10 = nn.Linear(8,1)\n",
    "    \n",
    "    def forward(self, inputs_A,inputs_B):\n",
    "        \n",
    "        \n",
    "        inputs_A = inputs_A.reshape(512,1,1024)\n",
    "        output_conv_A = self.relu(self.conv1(inputs_A))\n",
    "        output_conv_A = output_conv_A.reshape(512,33726) \n",
    "        output_A = self.relu(self.fc1(output_conv_A))\n",
    "        \n",
    "        inputs_B = inputs_B.reshape(512,1,1024)\n",
    "        output_conv_B = self.relu(self.conv1(inputs_B))\n",
    "        output_conv_B = output_conv_B.reshape(512,33726) \n",
    "        output_B = self.relu(self.fc1(output_conv_B))\n",
    "        \n",
    "        \n",
    "        output = torch.add(output_A, output_B)\n",
    "        for i in range(self.config.layer_fc_1):\n",
    "            output = self.relu(self.fully_connected_layers_1[i](output))\n",
    "        output = self.relu(self.fc_2(output))\n",
    "        for i in range(self.config.layer_fc_2):\n",
    "            output = self.relu(self.fully_connected_layers_2[i](output))\n",
    "            \n",
    "        output  = self.bn2(output)\n",
    "        output = self.relu(self.fc3(output))\n",
    "        if self.config.dropout:\n",
    "            output = self.drop(output)\n",
    "            \n",
    "        output = self.relu(self.fc4(output))\n",
    "        output = self.relu(self.fc5(output))\n",
    "        output = self.relu(self.fc6(output))\n",
    "        output = self.fc7(output)\n",
    "        output = self.fc8(output)\n",
    "        output = self.fc9(output)\n",
    "        output = self.fc10(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae8abe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(network, optimizer,learning_rate, momentum, weight_decay, amsgrad):\n",
    "    \n",
    "    \n",
    "#     if optimizer == \"sgd\":\n",
    "#         optimizer_ = optim.SGD(network.parameters(),\n",
    "#                               lr = learning_rate, momentum = momentum, weight_decay = weight_decay,\n",
    "#                               )\n",
    "        \n",
    "        \n",
    "    if optimizer == \"adam\":\n",
    "        optimizer_ = optim.Adam(network.parameters(),\n",
    "                               lr = learning_rate, betas = (0.9,0.999), weight_decay = weight_decay,\n",
    "                               amsgrad = amsgrad)\n",
    "        \n",
    "#     elif optimizer == \"rms_prop\":\n",
    "#         optimizer_ = optim.RMSprop(network.parameters(),\n",
    "#                                lr = learning_rate, alpha = 0.99, momentum = momentum,\n",
    "#                                   weight_decay = weight_decay)\n",
    "                               \n",
    "        \n",
    "    return optimizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "187891f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'amsgrad': False,\n",
      " 'dim_1': 2048,\n",
      " 'dim_2': 512,\n",
      " 'dropout': False,\n",
      " 'epochs': 500,\n",
      " 'layer_fc_1': 2,\n",
      " 'layer_fc_2': 2,\n",
      " 'learning_rate': 7e-05,\n",
      " 'momentum': 0.95,\n",
      " 'optimizer': 'adam',\n",
      " 'weight_decay': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# sweep_config = {\n",
    "#     'method': 'random'\n",
    "    \n",
    "#     }\n",
    "# metric = {\n",
    "#     'name': 'val_accuracy',\n",
    "#     'goal': 'maximize'   \n",
    "#     }\n",
    "# early_terminate = {\"type\": \"hyperband\",\n",
    "#       \"min_iter\": 3 }\n",
    "\n",
    "# sweep_config['metric'] = metric \n",
    "# sweep_config['early_terminate'] = early_terminate \n",
    "\n",
    "# parameters_dict = {\n",
    "    \n",
    "#     'layer_fc_1': {\n",
    "#         'values': [2]\n",
    "#         },\n",
    "   \n",
    "#     'dim_1': {\n",
    "#           'values': [2048]\n",
    "#         },\n",
    "    \n",
    "#     'layer_fc_2': {\n",
    "#         'values': [2]\n",
    "#         },\n",
    "#     'dim_2': {\n",
    "#           'values': [512]\n",
    "#         },\n",
    "    \n",
    "    \n",
    "#     'dropout': {\n",
    "#           'values': [False]\n",
    "#         },\n",
    "   \n",
    "    \n",
    "    \n",
    "#     'optimizer': {\n",
    "#           'values': ['adam']   #\n",
    "#         }\n",
    "#     ,\n",
    "  \n",
    "    \n",
    "#     'learning_rate': {\n",
    "#             'values':[0.0001]\n",
    "#         },\n",
    "    \n",
    "    \n",
    "#     'momentum': {\n",
    "#           'values': [0.95]\n",
    "#         },\n",
    "    \n",
    "#     'weight_decay': {\n",
    "#             'values': [0.009827436437331628]\n",
    "#         },\n",
    "   \n",
    "        \n",
    "#     'amsgrad': {\n",
    "#           'values': [False]\n",
    "#         },\n",
    "    \n",
    "    \n",
    "#     }\n",
    "\n",
    "\n",
    "# sweep_config['parameters'] = parameters_dict\n",
    "# parameters_dict.update({\n",
    "#     'epochs': {\n",
    "#         'value': 10000}\n",
    "#     })\n",
    "\n",
    "\n",
    "config = dict(layer_fc_1 = 2,\n",
    "        dim_1 = 2048,\n",
    "        layer_fc_2 = 2,\n",
    "        dim_2 = 512,\n",
    "        dropout = False,\n",
    "        optimizer = 'adam',\n",
    "        learning_rate = 0.00007,\n",
    "        momentum = 0.95,\n",
    "        weight_decay = 0.01,\n",
    "        amsgrad = False,\n",
    "        epochs = 500)\n",
    "\n",
    "\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e114cc",
   "metadata": {},
   "source": [
    "### 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "549fb3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "def train(config, train_dataloader,val_dataloader = None):\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Start training...\\n\")\n",
    "    epochs = config.epochs\n",
    "    \n",
    "    model = BertClassifier(config).to(device)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer,learning_rate, momentum, weight_decay, amsgrad = config.optimizer,config.learning_rate, config.momentum, config.weight_decay, config.amsgrad\n",
    "    optimizer = build_optimizer(model,optimizer,learning_rate, momentum, weight_decay, amsgrad)\n",
    "    \n",
    "    \n",
    "    for epoch_i in range(1,epochs+1):\n",
    "        \n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        for step,batch in tqdm(enumerate(train_dataloader)):\n",
    "            \n",
    "            inputs_A,inputs_B, b_labels = tuple(t.to(device) for t in batch)\n",
    "            b_labels = b_labels.reshape((1,512,1)).squeeze(0)\n",
    "            model.zero_grad()\n",
    "            logits = model(inputs_A,inputs_B)\n",
    "            loss = loss_fn(logits,b_labels.float()) \n",
    "            total_loss += loss.item()\n",
    "            loss.mean().backward()\n",
    "            optimizer.step()\n",
    "               \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        \n",
    "    \n",
    "        if val_dataloader is not None:\n",
    "                \n",
    "                val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "                if val_accuracy > best_accuracy:\n",
    "                    best_accuracy = val_accuracy\n",
    "                    torch.save({\n",
    "                        'epoch': epoch_i ,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': loss_fn,\n",
    "                        }, 'best_model_trained_fc_v2.pth')\n",
    "        \n",
    "        wandb.log({\n",
    "                    'epoch': epoch_i, \n",
    "                    \"train_loss\": avg_train_loss,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"val_accuracy\": val_accuracy\n",
    "                   })\n",
    "        print(f\"Epoch: {epoch_i} | Training Loss: {avg_train_loss}  | Validation Loss: {val_loss}  | Accuracy: {val_accuracy:.2f}\")\n",
    "        with open('result.txt', 'a') as f:\n",
    "            print(f\"Epoch: {epoch_i} | Training Loss: {avg_train_loss}  | Validation Loss: {val_loss}  | Accuracy: {val_accuracy:.2f}\", file=f) \n",
    "    print(\"\\n\")\n",
    "    wandb.log({\"val_accuracy\": best_accuracy,\n",
    "                   })\n",
    "    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n",
    "    \n",
    "\n",
    "def evaluate(model,val_dataloader):\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        inputs_A,inputs_B ,b_labels = tuple(t.to(device) for t in batch)\n",
    "        b_labels = b_labels.reshape((1,512,1)).squeeze(0)\n",
    "        with torch.no_grad():\n",
    "                logits = model(inputs_A,inputs_B)\n",
    "        \n",
    "        loss = loss_fn(logits, b_labels.float())\n",
    "        val_loss.append(loss.item())\n",
    "        preds = torch.round(torch.sigmoid(logits))\n",
    "        \n",
    "        accuracy = (preds.float() == b_labels.float()).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "    \n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68fcd769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    # Make the data\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print('The code uses GPU...')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print('The code uses CPU!!!')\n",
    "\n",
    "    \n",
    "    model = BertClassifier(config).to(device)\n",
    "\n",
    "    # Make the loss and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer,learning_rate, momentum, weight_decay, amsgrad = config.optimizer,config.learning_rate, config.momentum, config.weight_decay, config.amsgrad\n",
    "    optimizer = build_optimizer(model,optimizer,learning_rate, momentum, weight_decay, amsgrad)\n",
    "    \n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee1d948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(config=None):\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"Approach_2_elementwise_addition_siamese_network_on_sum_\", config=config):\n",
    "      # access all HPs through wandb.config, so logging matches execution!\n",
    "      config = wandb.config\n",
    "      \n",
    "      # make the model, data, and optimization problem\n",
    "      \n",
    "      \n",
    "      \n",
    "      # and use them to train the model\n",
    "      train(config, train_loader,val_dataloader = val_loader)\n",
    "     \n",
    "      for i,j in test_loader:\n",
    "          x = i\n",
    "      wandb.save(\"model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c4e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/anwer/Desktop/PPI_prediction/model_training_py/models/wandb/run-20221207_190123-3h65yii0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/danish2562022/Approach_2_elementwise_addition_siamese_network_on_sum_/runs/3h65yii0\" target=\"_blank\">iconic-snowball-1</a></strong> to <a href=\"https://wandb.ai/danish2562022/Approach_2_elementwise_addition_siamese_network_on_sum_\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "390it [01:01,  6.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████| 39/39 [00:01<00:00, 19.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training Loss: 0.5339629397942469  | Validation Loss: 0.5406321929051325  | Accuracy: 71.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "390it [01:00,  6.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████| 39/39 [00:02<00:00, 18.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Training Loss: 0.40738501365368185  | Validation Loss: 0.6404198438693316  | Accuracy: 67.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "390it [01:03,  6.16it/s]\n",
      "100%|██████████████████████████████████████████████████████████| 39/39 [00:02<00:00, 16.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Training Loss: 0.3706418472986955  | Validation Loss: 0.6481726475251026  | Accuracy: 72.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "390it [01:10,  5.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████| 39/39 [00:02<00:00, 14.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Training Loss: 0.3504295328488717  | Validation Loss: 0.5206643006740472  | Accuracy: 74.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "390it [01:14,  5.21it/s]\n",
      "100%|██████████████████████████████████████████████████████████| 39/39 [00:02<00:00, 14.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Training Loss: 0.33605266633706216  | Validation Loss: 0.5572626323272021  | Accuracy: 73.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "390it [01:17,  5.05it/s]\n",
      "100%|██████████████████████████████████████████████████████████| 39/39 [00:02<00:00, 14.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Training Loss: 0.3243838635774759  | Validation Loss: 0.5956513553093641  | Accuracy: 71.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "390it [01:17,  5.05it/s]\n",
      "100%|██████████████████████████████████████████████████████████| 39/39 [00:02<00:00, 14.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Training Loss: 0.3156016876300176  | Validation Loss: 0.6203331962609903  | Accuracy: 73.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "390it [01:17,  5.05it/s]\n",
      "100%|██████████████████████████████████████████████████████████| 39/39 [00:02<00:00, 14.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Training Loss: 0.310846257630067  | Validation Loss: 0.6118597693932362  | Accuracy: 72.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "390it [01:17,  5.05it/s]\n",
      "100%|██████████████████████████████████████████████████████████| 39/39 [00:02<00:00, 14.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Training Loss: 0.3046357437586173  | Validation Loss: 0.5179557555761093  | Accuracy: 74.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "390it [01:17,  5.05it/s]\n",
      "100%|██████████████████████████████████████████████████████████| 39/39 [00:02<00:00, 14.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Training Loss: 0.2995271167693994  | Validation Loss: 0.5856449298369579  | Accuracy: 72.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "390it [01:20,  4.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████| 39/39 [00:03<00:00,  9.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Training Loss: 0.29464834462373685  | Validation Loss: 0.6065697593566699  | Accuracy: 72.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "390it [01:21,  4.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████| 39/39 [00:03<00:00, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Training Loss: 0.290241258419477  | Validation Loss: 0.624604616409693  | Accuracy: 71.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95it [00:18,  5.05it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "model_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce2f691",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6f6413e",
   "metadata": {},
   "source": [
    "### 7. Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe6561",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
